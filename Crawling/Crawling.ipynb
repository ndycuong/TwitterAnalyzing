{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"knowledge_graph.projects.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET PROJECTS FROM FILE\n",
    "import json\n",
    "\n",
    "# Provide the correct file path\n",
    "file_path = r'knowledge_graph.projects.json'\n",
    "\n",
    "# Try with different encoding\n",
    "with open(file_path, 'r', encoding='latin-1') as file:\n",
    "    data_list = json.load(file)\n",
    "# Extract desired information for each element\n",
    "projects = []\n",
    "for data in data_list:\n",
    "    twitter_link = data.get(\"links\", {}).get(\"twitter\", \"\")\n",
    "    category= data.get(\"category\", \"\")\n",
    "    if twitter_link and category:\n",
    "        extracted_info = {\n",
    "            \"id\": data[\"_id\"],\n",
    "            \"name\": data.get(\"name\", \"\"),\n",
    "            \"link\": twitter_link,\n",
    "            \"category\": data.get(\"category\", \"\"),\n",
    "            \"description\": data.get(\"description\", \"\")\n",
    "        }\n",
    "        projects.append(extracted_info)\n",
    "\n",
    "# Save the extracted information to a new JSON file\n",
    "output_file_path = \"project_list.json\"\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(projects, output_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET FOLLOWERS FROM PROJECTS\n",
    "import asyncio\n",
    "from twscrape import API, gather\n",
    "from twscrape.logger import set_log_level\n",
    "import json\n",
    "\n",
    "async def main():\n",
    "    result_list = []\n",
    "\n",
    "    api = API()  # or API(\"path-to.db\") - default is `accounts.db`\n",
    "    \n",
    "    # ADD ACCOUNTS (for CLI usage see BELOW)\n",
    "    # await api.pool.add_account(\"ndycuong1\", \"123456coi\", \"nguyenduycuong1501@gmail.com\", \"15011990Ndyc\")\n",
    "\n",
    "    file_path = r'project_list_copy.json'\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        data_list = json.load(file)\n",
    "\n",
    "    for datalist in data_list:\n",
    "        data = {'username': {}, 'followers': []}  # Move inside the loop to create a new data dictionary for each user\n",
    "        twitter_link = datalist.get(\"link\", \"\")\n",
    "        username = twitter_link.split(\"/\")[-1]\n",
    "        data['username'] = username\n",
    "        data['link'] = twitter_link\n",
    "        if username:\n",
    "            try:\n",
    "                user_by_name = await api.user_by_login(username)\n",
    "                id = user_by_name.id\n",
    "                followerses = await gather(api.followers(id, limit=15))\n",
    "                for follower_data in followerses[1:20]:\n",
    "                    data['followers'].append({'id': follower_data.id, 'username': follower_data.username})\n",
    "                filename = \"project_followers.json\"\n",
    "                with open(filename, 'a', encoding='utf-8') as jsonfile:\n",
    "                    json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {username}: {e}\")\n",
    "            \n",
    "        \n",
    "\n",
    "        #result_list.append(data)\n",
    "\n",
    "    # change log level, default info\n",
    "    set_log_level(\"DEBUG\")\n",
    "\n",
    "    # Save data to JSON file\n",
    "    \n",
    "\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await (main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######PYTHON 3.9 - JUPYTER NOTEBOOK\n",
    "#crawl data profile X from user url\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "PATH = r'F:\\slide\\chromedriver-win64\\chromedriver.exe'\n",
    "#user\n",
    "#password\n",
    "\n",
    "# Read the JSON file\n",
    "with open(\"project_list.json\", 'r', encoding='latin-1') as file:\n",
    "    project_list = json.load(file)\n",
    "\n",
    "# Extracting username of projects\n",
    "\n",
    "\n",
    "urls=[]\n",
    "# Assuming project_list is a list of projects\n",
    "for project in project_list:\n",
    "\n",
    "    # Extract the desired value (Twitter handle) for each project\n",
    "    urls.append(project[\"link\"])\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(PATH)\n",
    "\n",
    "# Login to Twitter\n",
    "driver.get(\"https://twitter.com/login\")\n",
    "time.sleep(15)\n",
    "\n",
    "result_list = []\n",
    "\n",
    "for target_url in urls:\n",
    "    #url_parts = target_url.split(\"/\")\n",
    "\n",
    "# Get the last part of the URL\n",
    "    #username = url_parts[-1]\n",
    "    o = {}\n",
    "    twitte=[]\n",
    "    # Go to the profile page    \n",
    "    driver.get(target_url)\n",
    "    time.sleep(10)\n",
    "    resp = driver.page_source\n",
    "    \n",
    "    soup = BeautifulSoup(resp, 'html.parser')\n",
    "    o['link']=target_url\n",
    "\n",
    "# Define a regex pattern to extract name and username\n",
    "    \n",
    "#get intro\n",
    "    try:\n",
    "        profile_header = soup.find(\"div\", {\"data-testid\": \"UserProfileHeader_Items\"})\n",
    "    except:\n",
    "        None\n",
    "    try:\n",
    "        o[\"profile_joining_date\"] = profile_header.find(\"span\", {\"data-testid\": \"UserJoinDate\"}).text\n",
    "    except:\n",
    "        o[\"profile_joining_date\"] = None\n",
    "\n",
    "    try:\n",
    "        o[\"is_verifed\"] = bool(soup.find(\"svg\", {\"data-testid\": \"icon-verified\"}))\n",
    "    except:\n",
    "        o[\"is_verifed\"] = False\n",
    "\n",
    "\n",
    "    # Extract location information\n",
    "    try:\n",
    "        #location_section = soup.find(\"span\", {\"class\": \"css-901oao css-16my406 r-1bwzh9t r-4qtqp9 r-poiln3 r-1b7u577 r-bcqeeo r-qvutc0\"})[1].find(\"span\",{\"class\":\"css-901oao css-16my406 r-poiln3 r-bcqeeo r-qvutc0\"}).text\n",
    "        o[\"profile_location\"] = soup.find(\"span\", {\"data-testid\":\"UserLocation\"}).text\n",
    "            #     location_parts = location_section.find_all(\"span\", recursive=False)\n",
    "    #     o[\"profile_location\"] = location_parts[0].text.strip()\n",
    "    except:\n",
    "        o[\"profile_location\"] = None\n",
    "#get followers       \n",
    "    follower_url=target_url+'/followers'\n",
    "    driver.get(follower_url)\n",
    "    time.sleep(5)\n",
    "    scroll_down_count = 3  # Change this to the number of times you want to scroll down\n",
    "    for _ in range(scroll_down_count):\n",
    "    # Scroll down to the bottom of the page\n",
    "        #driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        #driver.execute_script(\"window.scrollBy(0, window.innerHeight);\")\n",
    "        scroll_distance = 2000\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_distance});\")\n",
    "\n",
    "        # Wait for a short time to let the page load more content\n",
    "        time.sleep(10)\n",
    "\n",
    "        resp = driver.page_source\n",
    "        soup = BeautifulSoup(resp, 'html.parser')\n",
    "    \n",
    "        block = soup.find_all(\"div\", {\"data-testid\":\"cellInnerDiv\"})\n",
    "        followers=[]\n",
    "    \n",
    "    \n",
    "        for user in block:\n",
    "            try:\n",
    "                #username = user.find(\"div\", {\"class\": \"css-1dbjc4n r-1adg3ll r-1ny4l3l\"}).find(\"a\", {\"class\": \"css-4rbku5 css-18t94o4 css-1dbjc4n r-1loqt21 r-1wbh5a2 r-dnmrzs r-1ny4l3l\"}).text\n",
    "                userurl = user.find(\"div\", {\"class\": \"css-1dbjc4n r-1adg3ll r-1ny4l3l\"}).find(\"div\", {\"class\": \"css-901oao css-1hf3ou5 r-1bwzh9t r-18u37iz r-37j5jr r-1wvb978 r-a023e6 r-16dba41 r-rjixqe r-bcqeeo r-qvutc0\"}).text\n",
    "                followers.append( userurl)\n",
    "            except:\n",
    "                pass       \n",
    "    o['followers']=followers\n",
    "\n",
    "    result_list.append(o)\n",
    "\n",
    "# Save data to JSON file\n",
    "    filename = \"project_followers.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(result_list, jsonfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Data saved to {filename}\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX FILE\n",
    "import json\n",
    "\n",
    "with open('profile_and_tweets_followers.json', 'r',encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Replace '}{' with '},{'\n",
    "corrected_data = data.replace('}{', '},{')\n",
    "\n",
    "with open('fixed_profile_and_tweets_followers.json', 'w',encoding='utf-8') as file:\n",
    "    file.write(corrected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from twscrape import API, gather\n",
    "from twscrape.logger import set_log_level\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class DateTimeEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, datetime):\n",
    "            return o.isoformat()\n",
    "        elif hasattr(o, '__dict__'):\n",
    "            return o.__dict__\n",
    "        return super().default(o)\n",
    "\n",
    "async def get_user_additional_info(api, user_id):\n",
    "    try:\n",
    "        user = await api.user_by_id(user_id)\n",
    "\n",
    "        if user is not None:\n",
    "            user_details = {\n",
    "                'id': user.id,\n",
    "                'username': user.username,\n",
    "                'displayName': user.displayname,\n",
    "                'url': user.url,\n",
    "                'intro': user.rawDescription,\n",
    "                'join_date': user.created,\n",
    "                'location': user.location,\n",
    "                'friends_count': user.friendsCount,\n",
    "                'favorite_count':user.favouritesCount,\n",
    "                'followers_count': user.followersCount,\n",
    "                'media_count': user.mediaCount,\n",
    "                'status_count': user.statusesCount,\n",
    "                'listed_count': user.listedCount,\n",
    "                'is_protected': user.protected,\n",
    "                'is_verified': user.verified,\n",
    "                'ref': user. descriptionLinks,\n",
    "            }\n",
    "            return user_details\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user details: {e}\")\n",
    "        return None\n",
    "\n",
    "async def crawl_user_profile_and_tweets(api, username, limit=20):\n",
    "    try:\n",
    "        # Get user information\n",
    "        user = await api.user_by_login(username)\n",
    "\n",
    "        if user is not None and hasattr(user, 'id'):\n",
    "            # Get user tweets\n",
    "            tweets = await gather(api.user_tweets(user.id, limit=limit))\n",
    "\n",
    "            # Create a list of dictionaries with user, tweet, and additional user information\n",
    "            data = {'user': {}, 'tweets': []}\n",
    "            user_additional_info = await get_user_additional_info(api, user.id)\n",
    "            if user_additional_info:\n",
    "                data['user'] = {\n",
    "                'id': user.id,\n",
    "                'username': user.username,\n",
    "                'displayName': user.displayname,\n",
    "                'url': user.url,\n",
    "                'intro': user.rawDescription,\n",
    "                'join_date': user.created,\n",
    "                'location': user.location,\n",
    "                'friends_count': user.friendsCount,\n",
    "                'favorite_count':user.favouritesCount,\n",
    "                'followers_count': user.followersCount,\n",
    "                'media_count': user.mediaCount,\n",
    "                'status_count': user.statusesCount,\n",
    "                'listed_count': user.listedCount,\n",
    "                'is_protected': user.protected,\n",
    "                'is_verified': user.verified,\n",
    "                'ref': user. descriptionLinks,\n",
    "                }\n",
    "\n",
    "            for tweet in tweets:\n",
    "                tweet_details = await api.tweet_details(tweet.id)\n",
    "                data['tweets'].append({\n",
    "                    'user':tweet.user.username,\n",
    "                    'tweet_id': tweet.id,\n",
    "                    'tweet_content': tweet.rawContent,\n",
    "                    'tweet_created_at': tweet.date,\n",
    "                    'like_count': tweet.likeCount,\n",
    "                    'retweet_count': tweet.retweetCount,\n",
    "                    'comment_count': tweet_details.replyCount,\n",
    "                    'view_count': tweet.viewCount ,\n",
    "                    'quote_count': tweet.quoteCount,\n",
    "                    'hashtags': tweet.hashtags,\n",
    "                    'place': tweet.place,\n",
    "                    'mentioned_users': tweet.mentionedUsers\n",
    "                })\n",
    "            \n",
    "            # Export data to a JSON file\n",
    "            filename = r\"profile_and_tweets_followers.json\"\n",
    "            with open(filename, 'a', encoding='utf-8') as file:\n",
    "                json.dump(data, file, ensure_ascii=False, indent=4, cls=DateTimeEncoder)\n",
    "\n",
    "            print(f\"Data for {username} has been successfully exported to file \")\n",
    "        else:\n",
    "            print(f\"User {username} not found or has no ID.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\") \n",
    "                 \n",
    "\n",
    "async def main():\n",
    "    api = API()\n",
    "    #await api.pool.add_account(\"your_username\", \"your_password\", \"your_email@example.com\", \"your_email_password\")\n",
    "    await api.pool.login_all()\n",
    "\n",
    "    json_file_path = r'part3.json'\n",
    "\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        user_list = json.load(file)\n",
    "        \n",
    "        \n",
    "\n",
    "    for username in user_list:\n",
    "    # Call the crawl_user_profile_and_tweets function\n",
    "        await crawl_user_profile_and_tweets(api, username, limit=20)\n",
    "\n",
    "    set_log_level(\"DEBUG\")\n",
    "\n",
    "    # Other actions you may want to perform using the API\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usernames of followers saved to project_follower_usernames.json\n"
     ]
    }
   ],
   "source": [
    "#CHECK DUPLICATE\n",
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "filename = \"project_followers_fixed.json\"\n",
    "with open(filename, 'r') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "# Extract unique usernames of followers\n",
    "follower_usernames = set()\n",
    "duplicate_usernames = set()\n",
    "for entry in data:\n",
    "    followers = entry.get(\"followers\", [])\n",
    "    for follower in followers:\n",
    "        username = follower.get(\"username\")\n",
    "        if username:\n",
    "            if username in follower_usernames:\n",
    "                duplicate_usernames.add(username)\n",
    "            else:\n",
    "                follower_usernames.add(username)\n",
    "\n",
    "# Save the unique usernames to a new JSON file\n",
    "output_filename = \"project_follower_usernames.json\"\n",
    "with open(output_filename, 'w') as output_file:\n",
    "    json.dump(list(follower_usernames), output_file, indent=4)\n",
    "\n",
    "print(f\"Usernames of followers saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed from the JSON file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File path of the JSON file\n",
    "file_path = \"Bigdata.json\"\n",
    "file_path1 = \"Bigdata1.json\"\n",
    "\n",
    "# Read the contents of the JSON file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Remove duplicates from the data\n",
    "data = list(set(json.dumps(item) for item in data))\n",
    "data = [json.loads(item) for item in data]\n",
    "\n",
    "# Write the updated data back to the JSON file\n",
    "with open(file_path1, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Duplicates removed from the JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "\n",
    "def process_user_objects(filename):\n",
    "    unique_users = {}\n",
    "    with open(filename, 'r',encoding=\"utf-8\") as file:\n",
    "        # ijson sẽ trả về mỗi đối tượng trong danh sách\n",
    "        for user_object in ijson.items(file, 'item'):\n",
    "            user_id = user_object['user']['id']\n",
    "            if user_id not in unique_users:\n",
    "                unique_users[user_id] = user_object\n",
    "\n",
    "    return list(unique_users.values())\n",
    "\n",
    "# Xử lý tệp JSON và nhận về danh sách không trùng lặp\n",
    "unique_user_objects = process_user_objects('Bigdata.json')\n",
    "\n",
    "# Ghi dữ liệu không trùng lặp vào một tệp mới\n",
    "with open('output_file.json', 'w',encoding=\"utf-8\") as file:\n",
    "    json.dump(unique_user_objects, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
