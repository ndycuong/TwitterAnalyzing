{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# twscrape add_accounts <file_path> <line_format>\n",
    "# line_format should have \"username\", \"password\", \"email\", \"email_password\" tokens\n",
    "# tokens delimeter should be same as an file\n",
    "twscrape add_accounts accounts.txt username:password:email:email_password\n",
    "twscrape login_accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install twscrape\n",
    "pip instal selenium\n",
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"User\"\n",
    "id: int\n",
    "    id_str: str\n",
    "    url: str\n",
    "    username: str\n",
    "    displayname: str\n",
    "    rawDescription: str\n",
    "    created: datetime\n",
    "    followersCount: int\n",
    "    friendsCount: int\n",
    "    statusesCount: int\n",
    "    favouritesCount: int\n",
    "    listedCount: int\n",
    "    mediaCount: int\n",
    "    location: str\n",
    "    profileImageUrl: str\n",
    "    profileBannerUrl: str | None = None\n",
    "    protected: bool | None = None\n",
    "    verified: bool | None = None\n",
    "    blue: bool | None = None\n",
    "    blueType: str | None = None\n",
    "    descriptionLinks: list[TextLink] = field(default_factory=list)\n",
    "    _type: str = \"snscrape.modules.twitter.User\"\n",
    "\n",
    "\"Tweets\"\n",
    "id: int\n",
    "    id_str: str\n",
    "    url: str\n",
    "    date: datetime\n",
    "    user: User\n",
    "    lang: str\n",
    "    rawContent: str\n",
    "    replyCount: int\n",
    "    retweetCount: int\n",
    "    likeCount: int\n",
    "    quoteCount: int\n",
    "    conversationId: int\n",
    "    hashtags: list[str]\n",
    "    cashtags: list[str]\n",
    "    mentionedUsers: list[UserRef]\n",
    "    links: list[TextLink]\n",
    "    viewCount: int | None = None\n",
    "    retweetedTweet: Optional[\"Tweet\"] = None\n",
    "    quotedTweet: Optional[\"Tweet\"] = None\n",
    "    place: Optional[Place] = None\n",
    "    coordinates: Optional[Coordinates] = None\n",
    "    inReplyToTweetId: int | None = None\n",
    "    inReplyToUser: UserRef | None = None\n",
    "    source: str | None = None\n",
    "    sourceUrl: str | None = None\n",
    "    sourceLabel: str | None = None\n",
    "    media: Optional[\"Media\"] = None\n",
    "    _type: str = \"snscrape.modules.twitter.Tweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#RUNNING IN JUPYTER NOTEBOOK\n",
    "#crawl data url from search X key\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import json\n",
    "\n",
    "#modify\n",
    "PATH = 'F:\\slide\\chromedriver-win64\\chromedriver.exe'\n",
    "username = \"CoiNguyn443321*\"\n",
    "password = \"123456coi\"\n",
    "\n",
    "# List of target URLs : tự cho\n",
    "urls = ['https://twitter.com/search?q=web3&src=typed_query&f=user','https://twitter.com/search?q=blockchain&src=typed_query&f=user','https://twitter.com/search?q=DeFi&src=typed_query&f=user'\n",
    ",'https://twitter.com/search?q=trend&src=typed_query&f=user', 'https://twitter.com/search?q=crypto%20musk&src=typeahead_click&f=user','https://twitter.com/search?q=Bitcoin&src=typeahead_click&f=user','https://twitter.com/search?q=dollar&src=typeahead_click&f=user','https://twitter.com/search?q=ukraine&src=typeahead_click&f=user']\n",
    "driver = webdriver.Chrome(PATH)\n",
    "\n",
    "# Login to Twitter\n",
    "driver.get(\"https://twitter.com/login\")\n",
    "time.sleep(15)\n",
    "\n",
    "result_list = []\n",
    "for target_url in urls:\n",
    "\n",
    "    driver.get(target_url)\n",
    "    time.sleep(10)\n",
    "    scroll_down_count = 5  # Change this to the number of times you want to scroll down\n",
    "    for _ in range(scroll_down_count):\n",
    "    # Scroll down to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait for a short time to let the page load more content\n",
    "        time.sleep(7)\n",
    "        resp = driver.page_source\n",
    "        soup = BeautifulSoup(resp, 'html.parser')\n",
    "    #find username\n",
    "        for user in soup.find_all(\"div\", {\"data-testid\":\"cellInnerDiv\"}):\n",
    "            try:\n",
    "                username = user.find(\"div\", {\"class\": \"css-1dbjc4n r-1adg3ll r-1ny4l3l\"}).find(\"a\", {\"class\": \"css-4rbku5 css-18t94o4 css-1dbjc4n r-1loqt21 r-1wbh5a2 r-dnmrzs r-1ny4l3l\"}).text\n",
    "                userurl = user.find(\"div\", {\"class\": \"css-1dbjc4n r-1adg3ll r-1ny4l3l\"}).find(\"div\", {\"class\": \"css-901oao css-1hf3ou5 r-1bwzh9t r-18u37iz r-37j5jr r-1wvb978 r-a023e6 r-16dba41 r-rjixqe r-bcqeeo r-qvutc0\"}).text\n",
    "            except:\n",
    "                pass\n",
    "            result_list.append({ \"username\": username, \"useurl\": userurl})\n",
    "\n",
    "    \n",
    "#     try:\n",
    "#         o['username'] = soup.find(\"div\", {\"class\": \"css-1dbjc4n r-1adg3ll r-1ny4l3l\"}).find(\"div\", {\"class\": \"css-1dbjc4n r-1wbh5a2 r-dnmrzs r-1ny4l3l\"}).text\n",
    "#     except:\n",
    "#         o[\"profile_name\"] = None\n",
    "    \n",
    "#     result_list.append(o)\n",
    "#   print(result_list)\n",
    "# Save data to JSON file\n",
    "   \n",
    "filename = \"twitter_profile_url10.json\"\n",
    "with open(filename, 'a', encoding='utf-8') as jsonfile:\n",
    "    json.dump(result_list, jsonfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"twitter_profile_url10.json\", encoding=\"utf-8\")\n",
    "\n",
    "# Loại bỏ các mục trùng lặp dựa trên cột \"useurl\"\n",
    "filenew = df.drop_duplicates(subset=\"useurl\", keep=\"first\")\n",
    "\n",
    "# Ghi DataFrame mới vào một tệp JSON\n",
    "filenew.to_json(\"profile_url.json\", orient=\"records\", force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for web3 has been successfully exported to file \n",
      "Data for polkastarter has been successfully exported to file \n",
      "Data for SecondLiveReal has been successfully exported to file \n",
      "Data for rebaked_dao has been successfully exported to file \n",
      "Data for mopindo has been successfully exported to file \n",
      "Data for MoonGirlBunny1 has been successfully exported to file \n",
      "Data for CAWATOKYO has been successfully exported to file \n",
      "Data for cengqi099 has been successfully exported to file \n",
      "Data for web3isgreat has been successfully exported to file \n",
      "Data for Web_3space has been successfully exported to file \n",
      "Data for AirdropAchiever has been successfully exported to file \n",
      "Data for okxweb3 has been successfully exported to file \n",
      "Data for BuyWeb3 has been successfully exported to file \n",
      "Data for taskonxyz has been successfully exported to file \n",
      "Data for web3_ready has been successfully exported to file \n",
      "Data for Web3Alerts has been successfully exported to file \n",
      "Data for IvanOnTech has been successfully exported to file \n",
      "Data for RunOnFlux has been successfully exported to file \n",
      "Data for Myria has been successfully exported to file \n",
      "Data for QuillAudits has been successfully exported to file \n",
      "Data for RangersProtocol has been successfully exported to file \n",
      "Data for parsiq_net has been successfully exported to file \n",
      "Data for 4everland_org has been successfully exported to file \n",
      "Data for Web3_Protocol has been successfully exported to file \n",
      "Data for Web3Capital has been successfully exported to file \n",
      "Data for torum_official has been successfully exported to file \n",
      "Data for AlchemyPlatform has been successfully exported to file \n",
      "Data for racefi_io has been successfully exported to file \n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from twscrape import API, gather\n",
    "from twscrape.logger import set_log_level\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class DateTimeEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, datetime):\n",
    "            return o.isoformat()\n",
    "        elif hasattr(o, '__dict__'):\n",
    "            return o.__dict__\n",
    "        return super().default(o)\n",
    "\n",
    "async def get_user_additional_info(api, user_id):\n",
    "    try:\n",
    "        user = await api.user_by_id(user_id)\n",
    "\n",
    "        if user is not None:\n",
    "            user_details = {\n",
    "                'id': user.id,\n",
    "                'username': user.username,\n",
    "                'displayName': user.displayname,\n",
    "                'url': user.url,\n",
    "                'intro': user.rawDescription,\n",
    "                'join_date': user.created,\n",
    "                'location': user.location,\n",
    "                'friends_count': user.friendsCount,\n",
    "                'favorite_count':user.favouritesCount,\n",
    "                'followers_count': user.followersCount,\n",
    "                'media_count': user.mediaCount,\n",
    "                'status_count': user.statusesCount,\n",
    "                'listed_count': user.listedCount,\n",
    "                'is_protected': user.protected,\n",
    "                'is_verified': user.verified,\n",
    "                'ref': user. descriptionLinks,\n",
    "            }\n",
    "            return user_details\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user details: {e}\")\n",
    "        return None\n",
    "\n",
    "async def crawl_user_profile_and_tweets(api, username, limit=20):\n",
    "    try:\n",
    "        # Get user information\n",
    "        user = await api.user_by_login(username)\n",
    "\n",
    "        if user is not None and hasattr(user, 'id'):\n",
    "            # Get user tweets\n",
    "            tweets = await gather(api.user_tweets(user.id, limit=limit))\n",
    "\n",
    "            # Create a list of dictionaries with user, tweet, and additional user information\n",
    "            data = {'user': {}, 'tweets': []}\n",
    "            user_additional_info = await get_user_additional_info(api, user.id)\n",
    "            if user_additional_info:\n",
    "                data['user'] = {\n",
    "                'id': user.id,\n",
    "                'username': user.username,\n",
    "                'displayName': user.displayname,\n",
    "                'url': user.url,\n",
    "                'intro': user.rawDescription,\n",
    "                'join_date': user.created,\n",
    "                'location': user.location,\n",
    "                'friends_count': user.friendsCount,\n",
    "                'favorite_count':user.favouritesCount,\n",
    "                'followers_count': user.followersCount,\n",
    "                'media_count': user.mediaCount,\n",
    "                'status_count': user.statusesCount,\n",
    "                'listed_count': user.listedCount,\n",
    "                'is_protected': user.protected,\n",
    "                'is_verified': user.verified,\n",
    "                'ref': user. descriptionLinks,\n",
    "                }\n",
    "\n",
    "            for tweet in tweets:\n",
    "                tweet_details = await api.tweet_details(tweet.id)\n",
    "                data['tweets'].append({\n",
    "                    'user':tweet.user.username,\n",
    "                    'tweet_id': tweet.id,\n",
    "                    'tweet_content': tweet.rawContent,\n",
    "                    'tweet_created_at': tweet.date,\n",
    "                    'like_count': tweet.likeCount,\n",
    "                    'retweet_count': tweet.retweetCount,\n",
    "                    'comment_count': tweet_details.replyCount,\n",
    "                    'view_count': tweet.viewCount ,\n",
    "                    'quote_count': tweet.quoteCount,\n",
    "                    'hashtags': tweet.hashtags,\n",
    "                    'place': tweet.place,\n",
    "                    'mentioned_users': tweet.mentionedUsers\n",
    "                })\n",
    "            \n",
    "            # Export data to a JSON file\n",
    "            filename = \"profile_and_tweets.json\"\n",
    "            with open(filename, 'a', encoding='utf-8') as file:\n",
    "                json.dump(data, file, ensure_ascii=False, indent=4, cls=DateTimeEncoder)\n",
    "\n",
    "            print(f\"Data for {username} has been successfully exported to file \")\n",
    "        else:\n",
    "            print(f\"User {username} not found or has no ID.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\") \n",
    "                 \n",
    "\n",
    "async def main():\n",
    "    api = API()\n",
    "    #await api.pool.add_account(\"your_username\", \"your_password\", \"your_email@example.com\", \"your_email_password\")\n",
    "    await api.pool.login_all()\n",
    "\n",
    "    json_file_path = r'C:\\Users\\nguye\\OneDrive - Hanoi University of Science and Technology\\code\\bigdata\\profile_url.json'\n",
    "\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        user_list = json.load(file)\n",
    "        \n",
    "\n",
    "    for username in user_list:\n",
    "    # Call the crawl_user_profile_and_tweets function\n",
    "        await crawl_user_profile_and_tweets(api, username['useurl'].lstrip('@'), limit=2)\n",
    "\n",
    "    set_log_level(\"DEBUG\")\n",
    "\n",
    "    # Other actions you may want to perform using the API\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
