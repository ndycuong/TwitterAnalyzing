{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "# # Download required NLTK datasets\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize tools\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Combine custom stopwords with NLTK's stopwords\n",
    "\n",
    "stopwords_set = set(nltk_stopwords.words('english'))\n",
    "\n",
    "def normalize(text):\n",
    "    # Normalize text\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\" \n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text= emoji_pattern.sub(r'', text)\n",
    "\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', 'USER', text)  # Replace @username\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', 'URL', text)  # Replace URLs\n",
    "    text = re.sub(r'\\n', ' ', text)  # Remove new line breaks\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    text = re.sub(' +', ' ', text.strip())  # Remove extra spaces\n",
    "    text = re.sub(r'\\.{3,}', '', text)  # Loại bỏ dấu ba chấm (...)\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1', text)  # Loại bỏ dấu câu lặp\n",
    "    text = re.sub(r'url','',text) # Loại bỏ url\n",
    "    text = re.sub(r'\\b(\\S*?)(.)\\2{2,}\\b', r'\\1\\2', text)  # Loại bỏ từ lặp\n",
    "    text = re.sub(r'([A-Za-z0-9])\\1+', r'\\1\\1', text)  # Loại bỏ ký tự lặp\n",
    "    \n",
    "    # Remove emoticons like :), :v, etc.\n",
    "    text = re.sub(r'[:;=]-?[)(DPpOo/\\|\\\\]', '', text)  # Basic emoticons\n",
    "    text = re.sub(r'[xX][Dd]', '', text)  # XD type emoticons\n",
    "    text = re.sub(r'\\^_?\\^', '', text)  # ^^ type emoticons\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = normalize(text)  # Normalize first\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stopwords_set]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(x) for x in tokens]  # Lemmatize\n",
    "\n",
    "    # Convert back to string\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)\n",
    "\n",
    "import json\n",
    "\n",
    "# Giả sử hàm clean đã được định nghĩa như trước đây\n",
    "\n",
    "def process_json(json_data):\n",
    "    for user_data in json_data:\n",
    "        if 'tweets' in user_data:\n",
    "            for tweet in user_data['tweets']:\n",
    "                if 'tweet_content' in tweet:\n",
    "                    tweet['tweet_content'] = clean(tweet['tweet_content'])\n",
    "\n",
    "        # Làm sạch phần giới thiệu của người dùng\n",
    "        user_intro = user_data['user'].get('intro', '')\n",
    "        user_data['user']['intro'] = clean(user_intro)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "# Đọc file JSON\n",
    "with open('Data.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def write_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Specify the file path for the new JSON file\n",
    "output_file_path = 'Data1.json'\n",
    "\n",
    "# Áp dụng hàm clean\n",
    "cleaned_data = process_json(data)\n",
    "write_json(cleaned_data, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gắn project tag cho từng user từ Data của centic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to read data from a JSON file\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to write data to a JSON file\n",
    "def write_json_file(data, file_path):\n",
    "    with open(file_path, 'w', encoding ='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Đọc dữ liệu từ các tệp JSON\n",
    "projects_data = read_json_file('project_list.json')\n",
    "project_followers_data = read_json_file('project_followers_fixed.json')\n",
    "followers_data = read_json_file('Bigdata.json')\n",
    "\n",
    "# Tạo từ điển để ánh xạ từ link sang tên project và từ tên project sang category\n",
    "link_to_project_name = {project['link']: project['name'] for project in projects_data}\n",
    "project_name_to_category = {project['name']: project['category'] for project in projects_data}\n",
    "\n",
    "# Hàm để trả về danh sách các tên project mà một follower theo dõi, loại bỏ trùng lặp\n",
    "def get_unique_followed_projects(follower_username):\n",
    "    followed_projects = set()\n",
    "    for project_follower in project_followers_data:\n",
    "        if follower_username in [follower['username'] for follower in project_follower['followers']]:\n",
    "            project_name = link_to_project_name.get(project_follower['link'], None)\n",
    "            if project_name:\n",
    "                followed_projects.add(project_name)\n",
    "    return list(followed_projects)\n",
    "\n",
    "# Hàm để lấy thông tin project-following và category tương ứng\n",
    "def get_projects_and_categories(followed_project_names):\n",
    "    projects_and_categories = []\n",
    "    for project_name in followed_project_names:\n",
    "        category = project_name_to_category.get(project_name, \"Not Found\")\n",
    "        projects_and_categories.append({'project': project_name, 'category': category})\n",
    "    return projects_and_categories\n",
    "\n",
    "# Cập nhật dữ liệu followers với danh sách các project theo dõi duy nhất và category tương ứng\n",
    "for follower in followers_data:\n",
    "    follower_username = follower['user']['username']\n",
    "    unique_projects = get_unique_followed_projects(follower_username)\n",
    "    projects_categories = get_projects_and_categories(unique_projects)\n",
    "    follower['projects_following'] = projects_categories\n",
    "\n",
    "# Ghi dữ liệu đã cập nhật vào tệp JSON mới\n",
    "write_json_file(followers_data, 'updated_followers_with_projects.json') // Data.json"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
